{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gdown\n",
    "!pip install --user matplotlib\n",
    "!pip show wordcloud\n",
    "!pip install requests\n",
    "!pip install pymystem3\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install Counter\n",
    "!pip install sklearn\n",
    "!pip install pyLDAvis\n",
    "!pip install --upgrade numpy scipy scikit-learn\n",
    "!pip install scipy \n",
    "!pip install spacy\n",
    "!pip install pyLDAvis\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc25fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import re\n",
    "import scipy\n",
    "import pyLDAvis\n",
    "import sklearn\n",
    "import gdown\n",
    "import requests\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pymystem3\n",
    "import spacy\n",
    "import math\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "from nltk import bigrams, trigrams\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08de388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "import pandas as pd\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "file_id = \"1AdFQ-uPMIAF8RK1-HuRJnO511P-Ac_6N\"  \n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = \"data/all_cases_unmarked.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "df = pd.read_csv(output)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47038abb",
   "metadata": {},
   "source": [
    "Очищаем и лемматизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "custom_stopwords = [\n",
    "    'фио', 'гггг', 'подсудимый', 'суд',\n",
    "    'изымать', 'согласно', 'наказание',\n",
    "    'потерпевший', 'показание', 'судебный',\n",
    "    'преступление', 'адрес', 'свидетель',\n",
    "    'свой', 'находиться', 'час', 'ход',\n",
    "    'дело', 'российский федерация'\n",
    "             ]\n",
    "all_stopwords = {word.lower() for word in russian_stopwords.union(custom_stopwords)}\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "   \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Лемматизация\n",
    "    words = text.split()\n",
    "    lemmatized_words = [mystem.lemmatize(word)[0] for word in words]\n",
    "    # Удаление стоп-слов\n",
    "    filtered_words = [word for word in lemmatized_words if word not in all_stopwords and word.strip()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "cleaned_df = df[[\"cleaned_text\"]].copy()\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \" \".join(cleaned_df[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acde4c",
   "metadata": {},
   "source": [
    "Считаем слова, символы и токены в целом и в среднем на один приговор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8315d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_counts = [len(word_tokenize(text)) for text in cleaned_df[\"cleaned_text\"]]\n",
    "avg_words = sum(word_counts) / len(word_counts) if word_counts else 0\n",
    "median_words = sorted(word_counts)[len(word_counts) // 2] if word_counts else 0\n",
    "\n",
    "print(\"Среднее количество слов:\", avg_words)\n",
    "print(\"Медианное количество слов:\", median_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "unique_token_counts = [len(set(word_tokenize(text))) for text in cleaned_df[\"cleaned_text\"]]\n",
    "\n",
    "avg_unique_tokens = sum(unique_token_counts) / len(unique_token_counts) if unique_token_counts else 0\n",
    "\n",
    "sorted_tokens = sorted(unique_token_counts)\n",
    "median_unique_tokens = sorted_tokens[len(sorted_tokens) // 2] if unique_token_counts else 0\n",
    "\n",
    "print(\"Среднее количество уникальных токенов:\", avg_unique_tokens)\n",
    "print(\"Медианное количество уникальных токенов:\", median_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts = [len(text) for text in cleaned_df[\"cleaned_text\"]]\n",
    "\n",
    "avg_chars = sum(char_counts) / len(char_counts) if char_counts else 0\n",
    "\n",
    "sorted_chars = sorted(char_counts)\n",
    "median_chars = sorted_chars[len(sorted_chars) // 2] if char_counts else 0\n",
    "\n",
    "print(\"Среднее количество символов:\", avg_chars)\n",
    "print(\"Медианное количество символов:\", median_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33310523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_measures(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    types = set(tokens)\n",
    "    N = len(tokens)\n",
    "    V = len(types)\n",
    "    \n",
    "    if N == 0:\n",
    "        return {'TTR': 0, 'RTTR': 0, 'CTTR': 0, 'Herdan_C': 0}\n",
    "    \n",
    "    try:\n",
    "        herdan_c = math.log(V) / math.log(N) if V > 1 and N > 1 else 0\n",
    "    except ZeroDivisionError:\n",
    "        herdan_c = 0\n",
    "\n",
    "    return {\n",
    "        'TTR': V / N,\n",
    "        'RTTR': V / math.sqrt(N),\n",
    "        'CTTR': V / math.sqrt(2 * N),\n",
    "        'Herdan_C': herdan_c\n",
    "    }\n",
    "cleaned_df = cleaned_df[cleaned_df[\"cleaned_text\"].str.strip().astype(bool)]\n",
    "\n",
    "metrics = cleaned_df['cleaned_text'].apply(lexical_measures)\n",
    "metrics_df = pd.DataFrame(metrics.tolist())\n",
    "cleaned_df_with_metrics = pd.concat([cleaned_df, metrics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Усредняем по датафрейму\n",
    "avg_metrics = cleaned_df_with_metrics[['TTR', 'RTTR', 'CTTR', 'Herdan_C']].mean()\n",
    "\n",
    "print(\"Средние значения по корпусу:\")\n",
    "print(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c1d59",
   "metadata": {},
   "source": [
    "Сохраняем в csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12614d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('cleaned_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
